# Prepare datasets for MM-OVOD (borrows and edits from Detic)

The basic training of our model uses [LVIS](https://www.lvisdataset.org/) (which uses [COCO](https://cocodataset.org/) images) and [ImageNet-21K](https://www.image-net.org/download.php). 
<!-- Optionally, we use [Objects365](https://www.objects365.org/) and [OpenImages (Challenge 2019 version)](https://storage.googleapis.com/openimages/web/challenge2019.html) for cross-dataset evaluation.  -->
Before starting processing, please download the (selected) datasets from the official websites and place or sim-link them under `${OVMR_ROOT}/datasets/` with details shown below.

```
${OVMR_ROOT}/datasets/
    metadata/
    lvis/
    coco/
    imagenet/
    VisualGenome/
```
`metadata/` is our preprocessed meta-data (included in the repo). See the below [section](#Metadata) for details.
Please follow the following instruction to pre-process individual datasets.

### COCO and LVIS

First, download COCO images and LVIS data place them in the following way:

```
lvis/
    lvis_v1_train.json
    lvis_v1_val.json
coco/
    train2017/
    val2017/
```

Next, prepare the open-vocabulary LVIS training set using 

```
python tools/remove_lvis_rare.py --ann datasets/lvis/lvis_v1_train.json
```

This will generate `datasets/lvis/lvis_v1_train_norare.json`.

### ImageNet-21K
#### Prepare from scratch
The imagenet folder should look like the below, after following the data-processing
[script](https://github.com/Alibaba-MIIL/ImageNet21K/blob/main/dataset_preprocessing/processing_script.sh) from ImageNet-21K Pretraining for the Masses ensuring to use the FALL 2011 version.
After this script has run, please rename folders to give the below structure:
```
imagenet/
    imagenet21k_P/
        train/
            n00005787/
                n00005787_*.JPEG
            n00006484/
                n00006484_*.JPEG
            ...
        val/
            n00005787/
                n00005787_*.JPEG
            n00006484/
                n00006484_*.JPEG
            ...
        imagenet21k_small_classes/
            n00004475/
                n00004475_*.JPEG
            n00006024/
                n00006024_*.JPEG
            ...
```

The subset of ImageNet that overlaps with LVIS (IN-L in the paper) will be created from this directory
structure.

~~~
cd ${OVMR_ROOT}/datasets/
mkdir imagenet/annotations
python tools/create_imagenetlvis_json.py --imagenet-path datasets/imagenet/imagenet21k_P --out-path datasets/imagenet/annotations/imagenet_lvis_image_info.json
~~~
This creates `datasets/imagenet/annotations/imagenet_lvis_image_info.json`.

#### **Utilize existing imagenet21k**
Since preprocessing the imagenet21k may be time-consuming, we provide instructions for conveniently using existing imagenet21k.

```
cd ${OVMR_ROOT}/datasets/

ln -s ${Existing_Dir_of_ImageNet21k} imagenet

mkdir ${OVMR_ROOT}/datasets/annotations

# download imagenet21k annotations from [link](https://drive.google.com/file/d/1suLWh-OeekLbdJOIuZbY7mxpGMY20cK-/view?usp=drive_link) and place it into `annotations`.

```


### VisualGenome

Some of our image exemplars are sourced from VisualGenome and so download the dataset ensuring the following
files are present with the below structure:
```
VisualGenome/
    VG_100K/
        *.jpg
    VG_100K_2/
        *.jpg
    objects.json
    image_data.json
```

### Metadata

```
metadata/
    lvis_v1_train_cat_info.json
    lvis_image_exemplar_dict_K-005_author.json
```

`lvis_v1_train_cat_info.json` is used by the Federated loss.
This is created by 
~~~
python tools/get_lvis_cat_info.py --ann datasets/lvis/lvis_v1_train.json
~~~

`lvis_image_exemplar_dict_K-005_author.json` is the dictionary of image exemplars for each LVIS class
used in the paper and produce our results.
One can create their own as follows:
~~~
python tools/sample_exemplars.py --lvis-ann-path datasets/lvis/lvis_v1_val.json --exemplar-dict-path datasets/metadata/exemplar_dict.json -K 5 --out-path datasets/metadata/lvis_image_exemplar_dict_final_own.json
~~~

`lvis_image_exemplar_dict_K-030_own.json ` is the dictionary of image examplars for finetuning visual token generator in OVMR. It's necessary because the detection head classifies the image cropped by detection boxes instead of full image. It's also generated by the python file `tools/sample_exemplars.py`, which is followed by `tools/save_LVIS_exemplars.py` to save the cropped images into `datasets`. 

### Collating Image Exemplars

We provide the exact image exemplars used (5 per LVIS category) in our results in the metadata folder defined
above.
However, if you wish to create your own, one first needs to create a full dictionary of exemplars for each
LVIS category.
This is done by:
~~~
python tools/collate_exemplar_dict.py --lvis-dir datasets/lvis --imagenet-dir datasets/imagenet/imagenet21k_P --visual-genome-dir datasets/VisualGenome --output-path datasets/metadata/exemplar_dict.json
~~~
This will create `datasets/metadata/exemplar_dict.json` which is a dictionary of exemplars with
at least 10 exemplars per LVIS category.